import { ChatGoogleGenerativeAI } from "@langchain/google-genai"; // Imports Google's Generative AI models for chat.
import { createStuffDocumentsChain } from "langchain/chains/combine_documents"; // Imports a chain for combining documents.
import { ChatPromptTemplate } from "@langchain/core/prompts"; // Imports a template for chat prompts.
import { createRetrievalChain } from "langchain/chains/retrieval"; // Imports a chain for document retrieval.
import { RunnableLambda } from "@langchain/core/runnables"; // Imports a utility to create runnable lambda functions.
import { customRetriever } from './setupData.js'; // Imports the custom retriever from setupData.js

// --- Initialize the Gemini models --- // This section initializes the AI models from Google Gemini.
const llm = new ChatGoogleGenerativeAI({
    modelName: "gemini-2.5-flash", // Specifies the Gemini model for chat interactions, chosen for its stability and performance.
});

let retrievalChain; // Declares a variable to hold the retrieval chain, which will be initialized later.

// Sets up the Retrieval-Augmented Generation (RAG) pipeline with Gemini AI.
export async function setupRag() {
    // Wraps the custom retriever in a RunnableLambda for use in Langchain chains.
    const retriever = RunnableLambda.from(customRetriever).withConfig({
        runName: "CustomRetriever",
    });

    // Defines the prompt template to be sent to the LLM model.
    // The context will be populated with documents retrieved by the retriever.
    const aiPromp = ChatPromptTemplate.fromTemplate(`
        Answer the user's question based only on the following context:
        If the context does not contain the answer, respond with "I don't know" or "The answer is not available in the provided documents".
        <context>
        {context}
        </context>

        Question: {input}
    `);

    // Creates a chain that combines the user's question with the retrieved documents and sends it to the AI model.
    const combineDocsChain = await createStuffDocumentsChain(
        { 
            llm, // The large language model to use.
            prompt: aiPromp // The prompt template for the LLM.
        });
    
    // Creates the main retrieval chain, which orchestrates document retrieval and combines them with the LLM.
    retrievalChain = await createRetrievalChain({
        retriever, // The document retriever.
        combineDocsChain, // The chain for combining documents and prompting the LLM.
    });
    
}

/**
 * Asks a question using the configured RAG pipeline.
 * @param {string} question - The user's question.
 * @returns {string} The answer generated by the RAG pipeline.
 * @throws {Error} If the RAG pipeline has not been set up yet.
 */
export async function askQuestion(question) {
    if (!retrievalChain) {
        throw new Error("RAG pipeline caught an error"); // Throws an error if the retrieval chain is not initialized.
    }
    const response = await retrievalChain.invoke({ input: question });// Invokes the retrieval chain with the user's question.
    return response.answer; // Extracts and returns the answer from the RAG response.
}
